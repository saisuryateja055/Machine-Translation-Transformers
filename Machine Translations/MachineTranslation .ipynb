{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eW25Fez7iHSJ",
        "outputId": "9990f1b7-1171-43d6-ce0f-2315680feefd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import time\n",
        "import spacy\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.nn.functional import pad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "# import portalocker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5K67y2ckCwYa",
        "outputId": "8e0c071a-f4e4-4fa9-8564-213edf59ffbe"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/english_telugu_data.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-046c44a79c21>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Read the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/english_telugu_data.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_telugu_english_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mtrain_data_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-046c44a79c21>\u001b[0m in \u001b[0;36mread_telugu_english_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Function to read your custom dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_telugu_english_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/english_telugu_data.txt'"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "# Function to read your custom dataset\n",
        "def read_telugu_english_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        raw_data = []\n",
        "        for line in file:\n",
        "            telugu_sentence, english_sentence = line.strip().split('++++$++++')\n",
        "            raw_data.append((telugu_sentence, english_sentence))\n",
        "    return raw_data\n",
        "\n",
        "# Splitting the dataset\n",
        "def split_dataset(data, train_split=0.7, val_split=0.15, test_split=0.15):\n",
        "    total_size = len(data)\n",
        "    train_size = int(total_size * train_split)\n",
        "    val_size = int(total_size * val_split)\n",
        "    test_size = total_size - train_size - val_size\n",
        "    train_data, remaining_data = random_split(data, [train_size, total_size - train_size])\n",
        "    val_data, test_data = random_split(remaining_data, [val_size, test_size])\n",
        "    return list(train_data), list(val_data), list(test_data)\n",
        "\n",
        "# Custom Dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def get_raw_texts(self):\n",
        "        return [(src, trg) for src, trg in self.data]\n",
        "\n",
        "# Define tokenizers\n",
        "tokenizer_te = get_tokenizer('basic_english')  # Replace with a suitable tokenizer for Telugu\n",
        "tokenizer_en = get_tokenizer('basic_english')  # Suitable tokenizer for English\n",
        "\n",
        "# Build vocabulary function\n",
        "def build_vocabulary(tokenizer, dataset, min_freq=2):\n",
        "    def yield_tokens(data):\n",
        "        for src, trg in data:\n",
        "            yield tokenizer(src)\n",
        "            yield tokenizer(trg)\n",
        "\n",
        "    vocab = build_vocab_from_iterator(yield_tokens(dataset.get_raw_texts()), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"], min_freq=min_freq)\n",
        "    vocab.set_default_index(vocab['<unk>'])  # Set default index for unknown tokens\n",
        "    return vocab\n",
        "\n",
        "# Read the dataset\n",
        "file_path = '/content/english_telugu_data.txt'\n",
        "raw_data = read_telugu_english_data(file_path)\n",
        "train_data_raw, val_data_raw, test_data_raw = split_dataset(raw_data)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CustomDataset(train_data_raw)\n",
        "valid_dataset = CustomDataset(val_data_raw)\n",
        "test_dataset = CustomDataset(test_data_raw)\n",
        "\n",
        "# Load vocabularies\n",
        "vocab_src = build_vocabulary(tokenizer_te, train_dataset)\n",
        "vocab_trg = build_vocabulary(tokenizer_en, train_dataset)\n",
        "\n",
        "# Batch generation function\n",
        "def generate_batch(data_batch):\n",
        "    de_batch, en_batch = [], []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for (de_item, en_item) in data_batch:\n",
        "        # Convert list of indices into tensors\n",
        "        de_indices = torch.tensor([vocab_src[token] for token in tokenizer_te(de_item)], dtype=torch.long)\n",
        "        en_indices = torch.tensor([vocab_trg[token] for token in tokenizer_en(en_item)], dtype=torch.long)\n",
        "\n",
        "        # Concatenate BOS, indices, EOS\n",
        "        de_temp = torch.cat([torch.tensor([vocab_src['<bos>']], dtype=torch.long), de_indices, torch.tensor([vocab_src['<eos>']], dtype=torch.long)], dim=0).to(device)\n",
        "        en_temp = torch.cat([torch.tensor([vocab_trg['<bos>']], dtype=torch.long), en_indices, torch.tensor([vocab_trg['<eos>']], dtype=torch.long)], dim=0).to(device)\n",
        "\n",
        "        # Pad sequences to ensure consistent length\n",
        "        padded_de = F.pad(de_temp, (0, 20 - len(de_temp)), value=vocab_src['<pad>'])\n",
        "        padded_en = F.pad(en_temp, (0, 20 - len(en_temp)), value=vocab_trg['<pad>'])\n",
        "\n",
        "        de_batch.append(padded_de)\n",
        "        en_batch.append(padded_en)\n",
        "\n",
        "    return torch.stack(de_batch), torch.stack(en_batch)\n",
        "\n",
        "\n",
        "# DataLoader setup\n",
        "BATCH_SIZE = 128\n",
        "train_iter = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=generate_batch)\n",
        "BOS_IDX = vocab_trg['<bos>']\n",
        "EOS_IDX = vocab_trg['<eos>']\n",
        "PAD_IDX = vocab_trg['<pad>']\n",
        "MAX_PADDING = 20\n",
        "BATCH_SIZE = 128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIwRHGTJgx2n"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size):\n",
        "    super().__init__()\n",
        "    self.lut = nn.Embedding(vocab_size, embed_size)\n",
        "    self.d_model=embed_size\n",
        "\n",
        "  def forward(self, x):\n",
        "    return (self.lut(x) * math.sqrt(self.d_model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbm1Y6IvhosA"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,dropout=0.1,max_len=5000):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "    pe=tc.zeros(max_len,d_model)\n",
        "    for k in np.arange(max_len):\n",
        "      for i in np.arange(d_model//2):\n",
        "        theta = k / (100** ((2*i)/d_model))\n",
        "\n",
        "\n",
        "        pe[k, 2*i] = math.sin(theta)\n",
        "\n",
        "\n",
        "        pe[k, 2*i+1] = math.cos(theta)\n",
        "        self.register_buffer(\"pe\",pe)\n",
        "  def forward(self,x):\n",
        "    x=x+self.pe[:x.size(1)].requires_grad_(False)\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH1o4C4ullOB"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_key = d_model // n_heads\n",
        "        self.wq = nn.Linear(d_model, d_model)\n",
        "        self.wk = nn.Linear(d_model, d_model)\n",
        "        self.wv = nn.Linear(d_model, d_model)\n",
        "        self.wo = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]\n",
        "        q = self.wq(query)\n",
        "        k = self.wk(key)\n",
        "        v = self.wv(value)\n",
        "\n",
        "\n",
        "\n",
        "        q = q.view(batch_size, -1, self.n_heads, self.d_key).permute(0, 2, 1, 3)\n",
        "        k = k.view(batch_size, -1, self.n_heads, self.d_key).permute(0, 2, 1, 3)\n",
        "        v = v.view(batch_size, -1, self.n_heads, self.d_key).permute(0, 2, 1, 3)\n",
        "\n",
        "        scaled_dot_prod = torch.matmul(q, k.permute(0, 1, 3, 2)) / math.sqrt(self.d_key)\n",
        "        if mask is not None:\n",
        "            scaled_dot_prod = scaled_dot_prod.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        attention_probs = torch.softmax(scaled_dot_prod, dim=-1)\n",
        "        A = torch.matmul(self.dropout(attention_probs), v)\n",
        "        A = A.permute(0, 2, 1, 3).contiguous()\n",
        "        A = A.view(batch_size, -1, self.n_heads * self.d_key)\n",
        "        output = self.wo(A)\n",
        "        return output,attention_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atrv4DZYnI8J"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.fc1=nn.Linear(d_model,d_ff)\n",
        "    self.fc2=nn.Linear(d_ff,d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "  def forward(self,x):\n",
        "    x=self.fc1(x)\n",
        "    x=F.relu(x)\n",
        "    x=self.dropout(x)\n",
        "    x=self.fc2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxnbIHzUpIkY"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,d_ff,dropout):\n",
        "    super().__init__()\n",
        "    self.attention=MultiHeadAttention(d_model,n_heads,dropout)\n",
        "    self.attn_layer_norm=nn.LayerNorm(d_model)\n",
        "    self.positionwise_ffn=PositionwiseFeedForward(d_model,d_ff,dropout)\n",
        "    self.ffn_layer_norm=nn.LayerNorm(d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,src,src_mask):\n",
        "    # print(\"encoder\")\n",
        "    _src,attn_probs=self.attention(src,src,src,src_mask)\n",
        "    src=self.attn_layer_norm(src+self.dropout(_src))\n",
        "    _src=self.positionwise_ffn(src)\n",
        "    src=self.ffn_layer_norm(src+self.dropout(_src))\n",
        "    return src,attn_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DExnkC7fqaYL"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,d_model,n_layers,n_heads,d_ff,dropout):\n",
        "    super().__init__()\n",
        "    self.layers=nn.ModuleList([EncoderLayer(d_model,n_heads,d_ff,dropout) for _ in range(n_layers)])\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,src,src_mask):\n",
        "    for layer in self.layers:\n",
        "      src,attn_probs=layer(src,src_mask)\n",
        "    self.attn_probs=attn_probs\n",
        "    return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfRng90-rPdb"
      },
      "outputs": [],
      "source": [
        "class DecodeLayer(nn.Module):\n",
        "  def __init__(self,d_model,n_heads,d_ff,dropout):\n",
        "    super().__init__()\n",
        "    self.masked_attention=MultiHeadAttention(d_model,n_heads,dropout)\n",
        "    self.masked_attn_layer_norm=nn.LayerNorm(d_model)\n",
        "    self.attention=MultiHeadAttention(d_model,n_heads,dropout)\n",
        "    self.attn_layer_norm=nn.LayerNorm(d_model)\n",
        "    self.positionwise_ffn=PositionwiseFeedForward(d_model,d_ff,dropout)\n",
        "    self.ffn_layer_norm=nn.LayerNorm(d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,trg,src,trg_mask,src_mask):\n",
        "    # print(\"Decoder\")\n",
        "    _trg,attn_probs=self.masked_attention(trg,trg,trg,trg_mask)\n",
        "    trg=self.masked_attn_layer_norm(trg+self.dropout(_trg))\n",
        "    _trg,attn_probs=self.attention(trg,src,src,src_mask)\n",
        "    trg=self.attn_layer_norm(trg+self.dropout(_trg))\n",
        "    _trg=self.positionwise_ffn(trg)\n",
        "    trg=self.ffn_layer_norm(trg+self.dropout(_trg))\n",
        "    return trg,attn_probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU2hXrib_QeY"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model,n_layers,n_heads,d_ffn,dropout):\n",
        "    super().__init__()\n",
        "    self.layers=nn.ModuleList([DecodeLayer(d_model,n_heads,d_ffn,dropout) for _ in range(n_layers)])\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.wo=nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,trg,src,trg_mask,src_mask):\n",
        "    for layer in self.layers:\n",
        "      trg,attn_probs=layer(trg,src,trg_mask,src_mask)\n",
        "    self.attn_probs=attn_probs\n",
        "    return self.wo(trg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgmQ16xg_xec"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,encoder,decoder,src_embed,trg_embed,src_pad_idx,trg_pad_idx,device):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.src_embed=src_embed\n",
        "    self.trg_embed=trg_embed\n",
        "    self.src_pad_idx=src_pad_idx\n",
        "    self.trg_pad_idx=trg_pad_idx\n",
        "    self.device=device\n",
        "\n",
        "  def make_src_mask(self,src):\n",
        "    src_mask=(src!=self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return src_mask.to(self.device)\n",
        "\n",
        "  def make_trg_mask(self,trg):\n",
        "    seq_len=trg.shape[1]\n",
        "    trg_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    trg_sub_mask=tc.tril(tc.ones((seq_len,seq_len))).to(self.device).bool()\n",
        "    trg_mask=trg_mask &trg_sub_mask\n",
        "    return trg_mask\n",
        "  def forward(self,src,trg):\n",
        "    src_mask=self.make_src_mask(src)\n",
        "    trg_mask=self.make_trg_mask(trg)\n",
        "    src=self.src_embed(src)\n",
        "    trg=self.trg_embed(trg)\n",
        "    src=self.encoder(src,src_mask)\n",
        "    output=self.decoder(trg,src,trg_mask,src_mask)\n",
        "    # print(f\"type decoder output {type(output)}\")\n",
        "    return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-wZskovASZz"
      },
      "outputs": [],
      "source": [
        "def make_model(device, src_vocab, trg_vocab, n_layers: int = 3, d_model: int = 256,\n",
        "               d_ffn: int = 2048, n_heads: int = 8, dropout: float = 0.1,\n",
        "               max_length: int = 5000):\n",
        "  encoder = Encoder(d_model, n_layers, n_heads, d_ffn, dropout)\n",
        "  decoder = Decoder(len(trg_vocab), d_model, n_layers, n_heads, d_ffn, dropout)\n",
        "  src_embed = Embeddings(len(src_vocab), d_model)\n",
        "  trg_embed = Embeddings(len(trg_vocab), d_model)\n",
        "  pos_enc = PositionalEncoding(d_model, dropout, max_length)\n",
        "  model = Transformer(encoder, decoder, nn.Sequential(src_embed, pos_enc),\n",
        "                      nn.Sequential(trg_embed, pos_enc),\n",
        "                      src_pad_idx=src_vocab.get_stoi()[\"<pad>\"],\n",
        "                      trg_pad_idx=trg_vocab.get_stoi()[\"<pad>\"],\n",
        "                      device=device)\n",
        "  for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "vAQooRn4S_RI",
        "outputId": "4d491b38-0ba2-4001-aa73-bbafca1eb0f8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'vocab_src' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a8e5cf740464>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model = make_model(device, vocab_src, vocab_trg,\n\u001b[0m\u001b[1;32m      5\u001b[0m                    \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                    d_ffn=512, max_length=50)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_src' is not defined"
          ]
        }
      ],
      "source": [
        "import torch as tc\n",
        "device = tc.device('cuda' if tc.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = make_model(device, vocab_src, vocab_trg,\n",
        "                   n_layers=3, n_heads=8, d_model=256,\n",
        "                   d_ffn=512, max_length=50)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TNdxYmdSS_mp"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s7285g7eaUJQ"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hxZubzxiaXKY"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "  # set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  # loop through each batch in the iterator\n",
        "  for i, batch in enumerate(iterator):\n",
        "\n",
        "    # set the source and target batches\n",
        "    src,trg = batch\n",
        "\n",
        "    # zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # logits for each output\n",
        "    logits = model(src, trg[:,:-1])\n",
        "\n",
        "    # expected output\n",
        "    expected_output = trg[:,1:]\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = criterion(logits.contiguous().view(-1, logits.shape[-1]),\n",
        "                    expected_output.contiguous().view(-1))\n",
        "\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # clip the weights\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "    # update the weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # update the loss\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  # return the average loss for the epoch\n",
        "  return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DvBvJHNLaa7f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  # set the model to evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  # evaluate without updating gradients\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # loop through each batch in the iterator\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "      # set the source and target batches\n",
        "      src, trg = batch\n",
        "\n",
        "\n",
        "      # logits for each output\n",
        "      logits = model(src, trg[:,:-1])\n",
        "\n",
        "      # expected output\n",
        "      expected_output = trg[:,1:]\n",
        "\n",
        "      # calculate the loss\n",
        "      loss = criterion(logits.contiguous().view(-1, logits.shape[-1]),\n",
        "                      expected_output.contiguous().view(-1))\n",
        "\n",
        "      # update the loss\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  # return the average loss for the epoch\n",
        "  return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "etoBd7cTac61"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr--To5zagRC",
        "outputId": "939993bb-8bf5-4493-b344-210af5b1a442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 1m 27s\n",
            "\tTrain Loss: 4.027 | Train PPL:  56.116\n",
            "\t Val. Loss: 2.702 |  Val. PPL:  14.910\n",
            "Epoch: 02 | Time: 1m 26s\n",
            "\tTrain Loss: 2.357 | Train PPL:  10.564\n",
            "\t Val. Loss: 1.754 |  Val. PPL:   5.779\n",
            "Epoch: 03 | Time: 1m 26s\n",
            "\tTrain Loss: 1.603 | Train PPL:   4.967\n",
            "\t Val. Loss: 1.323 |  Val. PPL:   3.755\n",
            "Epoch: 04 | Time: 1m 26s\n",
            "\tTrain Loss: 1.180 | Train PPL:   3.253\n",
            "\t Val. Loss: 1.112 |  Val. PPL:   3.039\n",
            "Epoch: 05 | Time: 1m 26s\n",
            "\tTrain Loss: 0.924 | Train PPL:   2.518\n",
            "\t Val. Loss: 1.015 |  Val. PPL:   2.760\n",
            "Epoch: 06 | Time: 1m 26s\n",
            "\tTrain Loss: 0.760 | Train PPL:   2.138\n",
            "\t Val. Loss: 0.953 |  Val. PPL:   2.595\n",
            "Epoch: 07 | Time: 1m 26s\n",
            "\tTrain Loss: 0.648 | Train PPL:   1.913\n",
            "\t Val. Loss: 0.922 |  Val. PPL:   2.513\n",
            "Epoch: 08 | Time: 1m 26s\n",
            "\tTrain Loss: 0.569 | Train PPL:   1.767\n",
            "\t Val. Loss: 0.895 |  Val. PPL:   2.448\n",
            "Epoch: 09 | Time: 1m 26s\n",
            "\tTrain Loss: 0.508 | Train PPL:   1.662\n",
            "\t Val. Loss: 0.879 |  Val. PPL:   2.408\n",
            "Epoch: 10 | Time: 1m 27s\n",
            "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
            "\t Val. Loss: 0.886 |  Val. PPL:   2.427\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS =10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# loop through each epoch\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  # calculate the train loss and update the parameters\n",
        "  train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "\n",
        "  # calculate the loss on the validation set\n",
        "  valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  # calculate how long the epoch took\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  # save the model when it performs better than the previous run\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), 'transformer-model_tel.pt')\n",
        "\n",
        "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "EnRyVGU2aivr",
        "outputId": "323a585b-1cf4-47a3-a7be-fe9eb0ca2f9e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6c2612f6811a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformer-model_tel.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# calculate the loss on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# load the weights\n",
        "model.load_state_dict(torch.load('transformer-model_tel.pt'))\n",
        "\n",
        "# calculate the loss on the test set\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcxXURzWpCre",
        "outputId": "5b54e7df-016a-4592-b503-65ee9a93fdae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source = ['<bos>', 'how', 'was', 'it', '?', '<eos>']\n",
            "target translation = ['అది', 'ఎలా', 'ఉంది', '?']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def translate_sentence(sentence, model, device, vocab_src, vocab_trg, tokenizer_te, max_length=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Check if the input is a string and tokenize accordingly\n",
        "    if isinstance(sentence, str):\n",
        "        # Tokenize the sentence using the Telugu tokenizer\n",
        "        tokens = tokenizer_te(sentence)\n",
        "        src = ['<bos>'] + [token.lower() for token in tokens] + ['<eos>']\n",
        "    else:\n",
        "        src = ['<bos>'] + sentence + ['<eos>']\n",
        "\n",
        "    # Map the tokens to their respective indices in the source vocabulary\n",
        "    src_indexes = [vocab_src[token] if token in vocab_src else vocab_src['<unk>'] for token in src]\n",
        "\n",
        "    # Convert the list of indices to a tensor and add a batch dimension\n",
        "    src_tensor = torch.tensor(src_indexes, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # Initialize the list of target indices with the index of '<bos>'\n",
        "    trg_indexes = [vocab_trg['<bos>']]\n",
        "\n",
        "    # Initialize the loop to generate tokens up to a maximum length\n",
        "    for i in range(max_length):\n",
        "        # Convert the current list of target indices to a tensor and add a batch dimension\n",
        "        trg_tensor = torch.tensor(trg_indexes, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Feed the source and target tensors to the model to get the logits\n",
        "            output = model(src_tensor, trg_tensor)\n",
        "            pred_token = output.argmax(2)[:, -1].item()\n",
        "\n",
        "            # Check if the predicted token is '<eos>' or the maximum length is reached\n",
        "            if pred_token == vocab_trg['<eos>'] or i == (max_length - 1):\n",
        "                # Convert indices to tokens\n",
        "                trg_tokens = [vocab_trg.lookup_token(index) for index in trg_indexes[1:]]  # Skip '<bos>'\n",
        "                return src, trg_tokens\n",
        "\n",
        "            # Append the predicted token to the list of target indices\n",
        "            trg_indexes.append(pred_token)\n",
        "\n",
        "# Example usage\n",
        "src_text = \"how was it?\"\n",
        "model = model  # Replace with your actual model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Assuming CUDA is available and appropriate\n",
        "src, trg_tokens = translate_sentence(src_text, model, device, vocab_src, vocab_trg, tokenizer_te)\n",
        "print(f'source = {src}')\n",
        "print(f'target translation = {trg_tokens}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWoWhg52TMhQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}